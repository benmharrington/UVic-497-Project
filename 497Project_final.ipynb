{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from spleeter.separator import Separator\n",
    "from spleeter.audio.adapter import get_default_audio_adapter\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "\n",
    "\n",
    "import madmom\n",
    "import statistics as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose directories and other audio signal variables\n",
    "path = 'playlists/playlist'\n",
    "srate = 44100\n",
    "audio_files = []\n",
    "audio_files_mono = []\n",
    "hopSize = 512\n",
    "bufSize = 1024\n",
    "m_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files from playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files from playlist\n",
    "\n",
    "audio_loader = get_default_audio_adapter()\n",
    "\n",
    "\n",
    "for i, filename in enumerate(glob.glob(os.path.join(path, '*'))):\n",
    "    #data, sr = librosa.core.load(filename, sr=srate)\n",
    "    \n",
    "    waveform, wf_srate = audio_loader.load(filename, sample_rate=srate)\n",
    "\n",
    "    audio_files.append(waveform)\n",
    "    \n",
    "    waveform_mono = waveform.sum(axis=1) / 2\n",
    "    audio_files_mono.append(waveform_mono)\n",
    "    \n",
    "num_files = len(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take tempo/beats for each track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "beats = []\n",
    "beats_lib = []\n",
    "downbeats = []\n",
    "song_data = {}\n",
    "proc = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=4, fps=100)\n",
    "\n",
    "for i, x in enumerate(audio_files_mono):   \n",
    "        \n",
    "    # finds the tempo and song length\n",
    "    tempo, beat_times = librosa.beat.beat_track(x, sr=srate, hop_length=hopSize, start_bpm=80, units='time')\n",
    "    if (tempo < 70):\n",
    "        tempo = tempo*2\n",
    "    if (tempo > 140):\n",
    "        tempo = tempo/2\n",
    "    \n",
    "    beats_lib.append(beat_times)\n",
    "    \n",
    "    song_length = len(audio_files[i])/srate\n",
    "    \n",
    "    act = madmom.features.downbeats.RNNDownBeatProcessor()(audio_files[i])\n",
    "    \n",
    "    db = proc(act)\n",
    "    \n",
    "    b = []\n",
    "    db_temp = []\n",
    "    for y in db:\n",
    "        b.append(y[0])\n",
    "        if y[1] == 1:\n",
    "            db_temp.append(y[0])\n",
    "    \n",
    "    beats.append(b)\n",
    "    downbeats.append(db_temp)\n",
    "    \n",
    "    #prediction = proc2(audio_files[i])\n",
    "    #key = madmom.features.key.key_prediction_to_label(prediction)\n",
    "    key = 'null'\n",
    "    \n",
    "    song_data[i] = {\n",
    "        'length': song_length,\n",
    "        'tempo' : tempo,\n",
    "        'track_num' : i+1,\n",
    "        'key' : key,\n",
    "        \n",
    "    } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'length': 215.64226757369613,\n",
       "  'tempo': 107.666015625,\n",
       "  'track_num': 1,\n",
       "  'key': 'null'},\n",
       " 1: {'length': 229.62068027210884,\n",
       "  'tempo': 123.046875,\n",
       "  'track_num': 2,\n",
       "  'key': 'null'},\n",
       " 2: {'length': 234.68975056689342,\n",
       "  'tempo': 107.666015625,\n",
       "  'track_num': 3,\n",
       "  'key': 'null'},\n",
       " 3: {'length': 169.87573696145125,\n",
       "  'tempo': 92.28515625,\n",
       "  'track_num': 4,\n",
       "  'key': 'null'},\n",
       " 4: {'length': 353.8957823129252,\n",
       "  'tempo': 99.38401442307692,\n",
       "  'track_num': 5,\n",
       "  'key': 'null'},\n",
       " 5: {'length': 243.85451247165534,\n",
       "  'tempo': 130.83465189873417,\n",
       "  'track_num': 6,\n",
       "  'key': 'null'},\n",
       " 6: {'length': 252.26013605442176,\n",
       "  'tempo': 137.8125,\n",
       "  'track_num': 7,\n",
       "  'key': 'null'},\n",
       " 7: {'length': 266.5404081632653,\n",
       "  'tempo': 72.78829225352112,\n",
       "  'track_num': 8,\n",
       "  'key': 'null'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finds mean tempo and stretches tracks accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a list of tempos for the tracks and a mean tempo\n",
    "tempo_list = []\n",
    "for i in range(num_files):\n",
    "    tempo_list.append(song_data[i].get('tempo'))\n",
    "\n",
    "mean_tempo = np.mean(tempo_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the stretch value factors\n",
    "\n",
    "for i, x in enumerate(audio_files_mono):\n",
    "    audio = audio_files[i]\n",
    "    stretch_value = mean_tempo/song_data[i].get('tempo')\n",
    "    song_data[i]['stretch_value'] = stretch_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the segments that will be used for the mashup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts a time in seconds to a specific sample\n",
    "def sec_convert(seconds):\n",
    "    samples = seconds*srate\n",
    "    return round(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_convert(samples):\n",
    "    seconds = samples/srate\n",
    "    return round(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_beat(timestamp, songindex):\n",
    "\n",
    "    for i, x in enumerate(beats[songindex]):\n",
    "        if (timestamp < x < timestamp + 5):\n",
    "            \n",
    "            return x\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    #found no suitable beats \n",
    "        \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_downbeat(timestamp, songindex):\n",
    "    db = downbeats[songindex][0]\n",
    "    count = 0\n",
    "    for x in (downbeats[songindex]):\n",
    "        if x < timestamp:\n",
    "            db = x\n",
    "\n",
    "    value = db\n",
    "    abs_diff_function = lambda list_value : abs(list_value - value)\n",
    "    closest_value = min(beats_lib[songindex], key=abs_diff_function)\n",
    "\n",
    "    return closest_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines segments at the beginning and the end of each song\n",
    "# after stretching these will be equal to m_length number of beats.\n",
    "# these values are returned as sample values, not time\n",
    "\n",
    "track_beginnings = []\n",
    "track_ends = []\n",
    "\n",
    "for i, x in enumerate(audio_files):\n",
    "    \n",
    "    s_v = song_data[i].get('stretch_value')\n",
    "    first_downbeat = int(round(sec_convert(choose_downbeat(downbeats[i][0], i))))\n",
    "    start_end = int(round(first_downbeat+(srate*(m_length*s_v))))\n",
    "    \n",
    "    track_beginnings.append(x[first_downbeat:start_end])\n",
    "    \n",
    "    m_temp = beats[i][-1] - (m_length*s_v)\n",
    "    end_start = int(round(sec_convert(choose_downbeat(m_temp, i))))\n",
    "    track_end = int(round(end_start+(srate*(m_length*s_v))))\n",
    "    \n",
    "    track_ends.append(x[end_start:track_end])\n",
    "    \n",
    "    song_data[i]['start_end'] = start_end\n",
    "    song_data[i]['end_start'] = end_start\n",
    "    song_data[i]['first_downbeat'] = first_downbeat\n",
    "    song_data[i]['track_end'] = track_end\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splits tracks to stems using spleeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = Separator('spleeter:4stems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_models\\\\4stems', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.7\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B90E251320>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\Ben\\Anaconda3\\lib\\site-packages\\spleeter\\model\\functions\\unet.py:30: The name tf.keras.initializers.he_uniform is deprecated. Please use tf.compat.v1.keras.initializers.he_uniform instead.\n",
      "\n",
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "INFO:tensorflow:Apply unet for drums_spectrogram\n",
      "INFO:tensorflow:Apply unet for bass_spectrogram\n",
      "INFO:tensorflow:Apply unet for other_spectrogram\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\Ben\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "WARNING:tensorflow:From C:\\Users\\Ben\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from pretrained_models\\4stems\\model\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: C:\\Users\\Ben\\AppData\\Local\\Temp\\serving\\temp-b'1595300244'\\saved_model.pb\n",
      "WARNING:tensorflow:From C:\\Users\\Ben\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\predictor\\saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Ben\\AppData\\Local\\Temp\\serving\\1595300244\\variables\\variables\n"
     ]
    }
   ],
   "source": [
    "t_begin_stems = {}\n",
    "t_end_stems = {}\n",
    "\n",
    "for i, x in enumerate(audio_files):\n",
    "    \n",
    "    prediction = separator.separate(track_beginnings[i])\n",
    "    \n",
    "    vocal = prediction.get('vocals')\n",
    "    t_voc = vocal.astype(np.float32)\n",
    "    bass = prediction.get('bass')\n",
    "    t_bass = bass.astype(np.float32)\n",
    "    other = prediction.get('other')\n",
    "    t_other = other.astype(np.float32)\n",
    "    drums = prediction.get('drums')\n",
    "    t_drums = drums.astype(np.float32) \n",
    "    \n",
    "    t_begin_stems[i] = {\n",
    "    'drums':t_drums.sum(axis=1) / 2, \n",
    "    'bass':t_bass.sum(axis=1) / 2, \n",
    "    'vocals':t_voc.sum(axis=1) / 2, \n",
    "    'other':t_other.sum(axis=1) / 2,\n",
    "    }\n",
    "    \n",
    "    prediction = separator.separate(track_ends[i])\n",
    "    \n",
    "    vocal = prediction.get('vocals')\n",
    "    t_voc = vocal.astype(np.float32)\n",
    "    bass = prediction.get('bass')\n",
    "    t_bass = bass.astype(np.float32)\n",
    "    other = prediction.get('other')\n",
    "    t_other = other.astype(np.float32)\n",
    "    drums = prediction.get('drums')\n",
    "    t_drums = drums.astype(np.float32) \n",
    "    \n",
    "    t_end_stems[i] = {\n",
    "    'drums':t_drums.sum(axis=1) / 2,\n",
    "    'bass':t_bass.sum(axis=1) / 2, \n",
    "    'vocals':t_voc.sum(axis=1) / 2, \n",
    "    'other':t_other.sum(axis=1) / 2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestretch stems/mono audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestretch stems for mashups\n",
    "t_begin_stems_stretched = {}\n",
    "t_end_stems_stretched = {}\n",
    "\n",
    "for i, x in enumerate(audio_files):\n",
    "    \n",
    "    s_value = song_data[i].get('stretch_value')\n",
    "    \n",
    "    s_drums = librosa.effects.time_stretch(t_begin_stems[i]['drums'], s_value)\n",
    "    s_bass = librosa.effects.time_stretch(t_begin_stems[i]['bass'], s_value)\n",
    "    s_voc = librosa.effects.time_stretch(t_begin_stems[i]['vocals'], s_value)\n",
    "    s_other = librosa.effects.time_stretch(t_begin_stems[i]['other'], s_value)\n",
    "    \n",
    "    t_begin_stems_stretched[i] = {\n",
    "    'drums':s_drums, \n",
    "    'bass':s_bass, \n",
    "    'vocals':s_voc, \n",
    "    'other':s_other,\n",
    "    }\n",
    "    \n",
    "    s_drums = librosa.effects.time_stretch(t_end_stems[i]['drums'], s_value)\n",
    "    s_bass = librosa.effects.time_stretch(t_end_stems[i]['bass'], s_value)\n",
    "    s_voc = librosa.effects.time_stretch(t_end_stems[i]['vocals'], s_value)\n",
    "    s_other = librosa.effects.time_stretch(t_end_stems[i]['other'], s_value)\n",
    "    \n",
    "    t_end_stems_stretched[i] = {\n",
    "    'drums':s_drums,\n",
    "    'bass':s_bass, \n",
    "    'vocals':s_voc, \n",
    "    'other':s_other,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestretch rest of tracks\n",
    "tracks_tstretched = []\n",
    "\n",
    "s_value = song_data[0].get('stretch_value')\n",
    "tracks_tstretched.append(librosa.effects.time_stretch(audio_files_mono[0][:song_data[0]['end_start']], s_value))\n",
    "\n",
    "for i in range(num_files-2):\n",
    "    s_value = song_data[i+1].get('stretch_value')\n",
    "    tracks_tstretched.append(librosa.effects.time_stretch(audio_files_mono[i+1][song_data[i+1]['start_end']:song_data[i+1]['end_start']], s_value))\n",
    "\n",
    "s_value = song_data[num_files-1].get('stretch_value')\n",
    "tracks_tstretched.append(librosa.effects.time_stretch(audio_files_mono[num_files-1][song_data[num_files-1]['start_end']:], s_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates mashups using stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_segments(median, stem):\n",
    "    \n",
    "    for i in range(num_files):\n",
    "        cur_length = len(t_end_stems_stretched[i][stem])\n",
    "        \n",
    "        if cur_length > median:\n",
    "            t_end_stems_stretched[i][stem] = t_end_stems_stretched[i][stem][:-1]\n",
    "        if cur_length < median:\n",
    "            l_s = t_end_stems_stretched[i][stem][-1]\n",
    "            t_end_stems_stretched[i][stem] = np.append(t_end_stems_stretched[i][stem], l_s)\n",
    "            \n",
    "            \n",
    "        cur_length = len(t_begin_stems_stretched[i][stem])\n",
    "        if cur_length > median:\n",
    "            t_begin_stems_stretched[i][stem] = t_begin_stems_stretched[i][stem][:-1]\n",
    "            \n",
    "        if cur_length < median:\n",
    "            l_s = t_begin_stems_stretched[i][stem][-1]\n",
    "            t_begin_stems_stretched[i][stem] = np.append(t_begin_stems_stretched[i][stem], l_s)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESIZE STEMS SO ALL THE SAME SIZE\n",
    "\n",
    "lengths = []\n",
    "for i in range(num_files):\n",
    "    length = len(t_end_stems_stretched[i]['vocals'])\n",
    "    lengths.append(length)\n",
    "median = int(stats.median(lengths))\n",
    "\n",
    "resize_segments(median, 'drums')\n",
    "resize_segments(median, 'vocals')\n",
    "resize_segments(median, 'bass')\n",
    "resize_segments(median, 'other')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mashups consist of drums from current track/instrumental from next track\n",
    "mashups = []\n",
    "for i in range(num_files-1):\n",
    "\n",
    "    stem1 = t_end_stems_stretched[i]['drums']\n",
    "    stem2 = t_begin_stems_stretched[i+1]['vocals']\n",
    "    stem3 = t_begin_stems_stretched[i+1]['other']\n",
    "    stem4 = t_begin_stems_stretched[i+1]['bass']\n",
    "\n",
    "    mashup = (stem1 + stem2 + stem3 + stem4)\n",
    "\n",
    "    mashups.append(mashup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CrossFading Function, takes signal arrays s1, s2, and fadetime f and delay time d in samples\n",
    "#Written by Sarah Staszkiel\n",
    "\n",
    "def crossfade(s1,s2,f,d):\n",
    "    \n",
    "    #Add padded 0's to delay overlap of signals\n",
    "    a = np.pad(s1[(len(s1)-f):], (0, d), 'constant', constant_values=(0, 0))\n",
    "    b = np.pad(s2[:f], (d, 0), 'constant', constant_values=(0, 0))\n",
    "    c = []\n",
    "    l = f+d\n",
    "    \n",
    "    #Fade out s1 and fade in s2\n",
    "    for i in range(0, l):\n",
    "        m = i/f\n",
    "        a[i] = a[i]*(1-m) #Decreases from 1 to 0 over fade duration\n",
    "        b[(l-1)-i] = b[(l-1)-i]*(1-m) #Increase from 0 to 1 over fade duration\n",
    "        \n",
    "    a = a + b #Overlap both faded signals\n",
    "    c = np.concatenate((s1[:len(s1)-f],a,s2[f:]), axis=0)\n",
    "   \n",
    "    #For testing\n",
    "    #ipd.display(ipd.Audio(c,rate=srate))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_fade = mean_tempo/60\n",
    "delay = 0.1 # should be less than cross_fade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "crsf = int(sec_convert(cross_fade))\n",
    "d = sec_convert(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_file = tracks_tstretched[0]\n",
    "playlist_file = crossfade(playlist_file,mashups[0],crsf,d)\n",
    "\n",
    "for i in range(num_files-2):\n",
    "    \n",
    "    playlist_file = crossfade(playlist_file, tracks_tstretched[i+1],crsf, d)\n",
    "    playlist_file = crossfade(playlist_file, mashups[i+1], crsf, d)\n",
    "\n",
    "playlist_file = crossfade(playlist_file, tracks_tstretched[num_files-1], crsf, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "playlist_array = np.asarray(playlist_file)\n",
    "wav.write('test_playlist_lib.wav', srate, playlist_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END ------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
