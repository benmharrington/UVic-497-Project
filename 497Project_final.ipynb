{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from spleeter.separator import Separator\n",
    "from spleeter.audio.adapter import get_default_audio_adapter\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "\n",
    "\n",
    "import madmom\n",
    "import statistics as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose directories and other audio signal variables\n",
    "path = 'playlists/playlist'\n",
    "srate = 44100\n",
    "audio_files = []\n",
    "audio_files_mono = []\n",
    "hopSize = 512\n",
    "bufSize = 1024\n",
    "m_length = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files from playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files from playlist\n",
    "\n",
    "audio_loader = get_default_audio_adapter()\n",
    "\n",
    "\n",
    "for i, filename in enumerate(glob.glob(os.path.join(path, '*'))):\n",
    "    #data, sr = librosa.core.load(filename, sr=srate)\n",
    "    \n",
    "    waveform, wf_srate = audio_loader.load(filename, sample_rate=srate)\n",
    "\n",
    "    audio_files.append(waveform)\n",
    "    \n",
    "    waveform_mono = waveform.sum(axis=1) / 2\n",
    "    audio_files_mono.append(waveform_mono)\n",
    "    \n",
    "num_files = len(audio_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take tempo/beats for each track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beats = []\n",
    "beats_lib = []\n",
    "downbeats = []\n",
    "song_data = {}\n",
    "proc = madmom.features.downbeats.DBNDownBeatTrackingProcessor(beats_per_bar=4, fps=100)\n",
    "\n",
    "for i, x in enumerate(audio_files_mono):   \n",
    "        \n",
    "    # finds the tempo and song length\n",
    "    tempo, beat_times = librosa.beat.beat_track(x, sr=srate, hop_length=hopSize, start_bpm=80, units='time')\n",
    "    if (tempo < 70):\n",
    "        tempo = tempo*2\n",
    "    if (tempo > 140):\n",
    "        tempo = tempo/2\n",
    "    \n",
    "    beats_lib.append(beat_times)\n",
    "    \n",
    "    song_length = len(audio_files[i])/srate\n",
    "    \n",
    "    act = madmom.features.downbeats.RNNDownBeatProcessor()(audio_files[i])\n",
    "    \n",
    "    db = proc(act)\n",
    "    \n",
    "    b = []\n",
    "    db_temp = []\n",
    "    for y in db:\n",
    "        b.append(y[0])\n",
    "        if y[1] == 1:\n",
    "            db_temp.append(y[0])\n",
    "    \n",
    "    beats.append(b)\n",
    "    downbeats.append(db_temp)\n",
    "    \n",
    "    #prediction = proc2(audio_files[i])\n",
    "    #key = madmom.features.key.key_prediction_to_label(prediction)\n",
    "    key = 'null'\n",
    "    \n",
    "    song_data[i] = {\n",
    "        'length': song_length,\n",
    "        'tempo' : tempo,\n",
    "        'track_num' : i+1,\n",
    "        'key' : key,\n",
    "        \n",
    "    } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finds mean tempo and stretches tracks accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a list of tempos for the tracks and a mean tempo\n",
    "tempo_list = []\n",
    "for i in range(num_files):\n",
    "    tempo_list.append(song_data[i].get('tempo'))\n",
    "\n",
    "mean_tempo = np.mean(tempo_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the stretch value factors\n",
    "\n",
    "for i, x in enumerate(audio_files_mono):\n",
    "    audio = audio_files[i]\n",
    "    stretch_value = mean_tempo/song_data[i].get('tempo')\n",
    "    song_data[i]['stretch_value'] = stretch_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the segments that will be used for the mashup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts a time in seconds to a specific sample\n",
    "def sec_convert(seconds):\n",
    "    samples = seconds*srate\n",
    "    return round(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_convert(samples):\n",
    "    seconds = samples/srate\n",
    "    return round(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_beat(timestamp, songindex):\n",
    "\n",
    "    for i, x in enumerate(beats[songindex]):\n",
    "        if (timestamp < x < timestamp + 5):\n",
    "            \n",
    "            return x\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    #found no suitable beats \n",
    "        \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_downbeat(timestamp, songindex):\n",
    "    db = downbeats[songindex][0]\n",
    "    count = 0\n",
    "    for x in (downbeats[songindex]):\n",
    "        if x < timestamp:\n",
    "            db = x\n",
    "\n",
    "    value = db\n",
    "    abs_diff_function = lambda list_value : abs(list_value - value)\n",
    "    closest_value = min(beats_lib[songindex], key=abs_diff_function)\n",
    "\n",
    "    return closest_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines segments at the beginning and the end of each song\n",
    "# after stretching these will be equal to m_length number of beats.\n",
    "# these values are returned as sample values, not time\n",
    "\n",
    "track_beginnings = []\n",
    "track_ends = []\n",
    "\n",
    "for i, x in enumerate(audio_files):\n",
    "    \n",
    "    s_v = song_data[i].get('stretch_value')\n",
    "    first_downbeat = int(round(sec_convert(choose_downbeat(downbeats[i][0], i))))\n",
    "    start_end = int(round(first_downbeat+(srate*(m_length*s_v))))\n",
    "    \n",
    "    track_beginnings.append(x[first_downbeat:start_end])\n",
    "    \n",
    "    m_temp = beats[i][-1] - (m_length*s_v)\n",
    "    end_start = int(round(sec_convert(choose_downbeat(m_temp, i))))\n",
    "    track_end = int(round(end_start+(srate*(m_length*s_v))))\n",
    "    \n",
    "    track_ends.append(x[end_start:track_end])\n",
    "    \n",
    "    song_data[i]['start_end'] = start_end\n",
    "    song_data[i]['end_start'] = end_start\n",
    "    song_data[i]['first_downbeat'] = first_downbeat\n",
    "    song_data[i]['track_end'] = track_end\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splits tracks to stems using spleeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = Separator('spleeter:4stems')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_begin_stems = {}\n",
    "t_end_stems = {}\n",
    "\n",
    "for i, x in enumerate(audio_files):\n",
    "    \n",
    "    prediction = separator.separate(track_beginnings[i])\n",
    "    \n",
    "    vocal = prediction.get('vocals')\n",
    "    t_voc = vocal.astype(np.float32)\n",
    "    bass = prediction.get('bass')\n",
    "    t_bass = bass.astype(np.float32)\n",
    "    other = prediction.get('other')\n",
    "    t_other = other.astype(np.float32)\n",
    "    drums = prediction.get('drums')\n",
    "    t_drums = drums.astype(np.float32) \n",
    "    \n",
    "    t_begin_stems[i] = {\n",
    "    'drums':t_drums.sum(axis=1) / 2, \n",
    "    'bass':t_bass.sum(axis=1) / 2, \n",
    "    'vocals':t_voc.sum(axis=1) / 2, \n",
    "    'other':t_other.sum(axis=1) / 2,\n",
    "    }\n",
    "    \n",
    "    prediction = separator.separate(track_ends[i])\n",
    "    \n",
    "    vocal = prediction.get('vocals')\n",
    "    t_voc = vocal.astype(np.float32)\n",
    "    bass = prediction.get('bass')\n",
    "    t_bass = bass.astype(np.float32)\n",
    "    other = prediction.get('other')\n",
    "    t_other = other.astype(np.float32)\n",
    "    drums = prediction.get('drums')\n",
    "    t_drums = drums.astype(np.float32) \n",
    "    \n",
    "    t_end_stems[i] = {\n",
    "    'drums':t_drums.sum(axis=1) / 2,\n",
    "    'bass':t_bass.sum(axis=1) / 2, \n",
    "    'vocals':t_voc.sum(axis=1) / 2, \n",
    "    'other':t_other.sum(axis=1) / 2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestretch stems/mono audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestretch stems for mashups\n",
    "t_begin_stems_stretched = {}\n",
    "t_end_stems_stretched = {}\n",
    "\n",
    "for i, x in enumerate(audio_files):\n",
    "    \n",
    "    s_value = song_data[i].get('stretch_value')\n",
    "    \n",
    "    s_drums = librosa.effects.time_stretch(t_begin_stems[i]['drums'], s_value)\n",
    "    s_bass = librosa.effects.time_stretch(t_begin_stems[i]['bass'], s_value)\n",
    "    s_voc = librosa.effects.time_stretch(t_begin_stems[i]['vocals'], s_value)\n",
    "    s_other = librosa.effects.time_stretch(t_begin_stems[i]['other'], s_value)\n",
    "    \n",
    "    t_begin_stems_stretched[i] = {\n",
    "    'drums':s_drums, \n",
    "    'bass':s_bass, \n",
    "    'vocals':s_voc, \n",
    "    'other':s_other,\n",
    "    }\n",
    "    \n",
    "    s_drums = librosa.effects.time_stretch(t_end_stems[i]['drums'], s_value)\n",
    "    s_bass = librosa.effects.time_stretch(t_end_stems[i]['bass'], s_value)\n",
    "    s_voc = librosa.effects.time_stretch(t_end_stems[i]['vocals'], s_value)\n",
    "    s_other = librosa.effects.time_stretch(t_end_stems[i]['other'], s_value)\n",
    "    \n",
    "    t_end_stems_stretched[i] = {\n",
    "    'drums':s_drums,\n",
    "    'bass':s_bass, \n",
    "    'vocals':s_voc, \n",
    "    'other':s_other,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestretch rest of tracks\n",
    "tracks_tstretched = []\n",
    "\n",
    "s_value = song_data[0].get('stretch_value')\n",
    "tracks_tstretched.append(librosa.effects.time_stretch(audio_files_mono[0][:song_data[0]['end_start']], s_value))\n",
    "\n",
    "for i in range(num_files-2):\n",
    "    s_value = song_data[i+1].get('stretch_value')\n",
    "    tracks_tstretched.append(librosa.effects.time_stretch(audio_files_mono[i+1][song_data[i+1]['start_end']:song_data[i+1]['end_start']], s_value))\n",
    "\n",
    "s_value = song_data[num_files-1].get('stretch_value')\n",
    "tracks_tstretched.append(librosa.effects.time_stretch(audio_files_mono[num_files-1][song_data[num_files-1]['start_end']:], s_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creates mashups using stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_segments(median, stem):\n",
    "    \n",
    "    for i in range(num_files):\n",
    "        cur_length = len(t_end_stems_stretched[i][stem])\n",
    "        \n",
    "        if cur_length > median:\n",
    "            t_end_stems_stretched[i][stem] = t_end_stems_stretched[i][stem][:-1]\n",
    "        if cur_length < median:\n",
    "            l_s = t_end_stems_stretched[i][stem][-1]\n",
    "            t_end_stems_stretched[i][stem] = np.append(t_end_stems_stretched[i][stem], l_s)\n",
    "            \n",
    "            \n",
    "        cur_length = len(t_begin_stems_stretched[i][stem])\n",
    "        if cur_length > median:\n",
    "            t_begin_stems_stretched[i][stem] = t_begin_stems_stretched[i][stem][:-1]\n",
    "            \n",
    "        if cur_length < median:\n",
    "            l_s = t_begin_stems_stretched[i][stem][-1]\n",
    "            t_begin_stems_stretched[i][stem] = np.append(t_begin_stems_stretched[i][stem], l_s)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESIZE STEMS SO ALL THE SAME SIZE\n",
    "\n",
    "lengths = []\n",
    "for i in range(num_files):\n",
    "    length = len(t_end_stems_stretched[i]['vocals'])\n",
    "    lengths.append(length)\n",
    "median = int(stats.median(lengths))\n",
    "\n",
    "resize_segments(median, 'drums')\n",
    "resize_segments(median, 'vocals')\n",
    "resize_segments(median, 'bass')\n",
    "resize_segments(median, 'other')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mashups consist of drums from current track/instrumental from next track\n",
    "mashups = []\n",
    "for i in range(num_files-1):\n",
    "\n",
    "    stem1 = t_end_stems_stretched[i]['drums']\n",
    "    stem2 = t_begin_stems_stretched[i+1]['vocals']\n",
    "    stem3 = t_begin_stems_stretched[i+1]['other']\n",
    "    stem4 = t_begin_stems_stretched[i+1]['bass']\n",
    "\n",
    "    mashup = (stem1 + stem2 + stem3 + stem4)\n",
    "\n",
    "    mashups.append(mashup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CrossFading Function, takes signal arrays s1, s2, and fadetime f and delay time d in samples\n",
    "#Written by Sarah Staszkiel\n",
    "\n",
    "def crossfade(s1,s2,f,d):\n",
    "    \n",
    "    #Add padded 0's to delay overlap of signals\n",
    "    a = np.pad(s1[(len(s1)-f):], (0, d), 'constant', constant_values=(0, 0))\n",
    "    b = np.pad(s2[:f], (d, 0), 'constant', constant_values=(0, 0))\n",
    "    c = []\n",
    "    l = f+d\n",
    "    \n",
    "    #Fade out s1 and fade in s2\n",
    "    for i in range(0, l):\n",
    "        m = i/f\n",
    "        a[i] = a[i]*(1-m) #Decreases from 1 to 0 over fade duration\n",
    "        b[(l-1)-i] = b[(l-1)-i]*(1-m) #Increase from 0 to 1 over fade duration\n",
    "        \n",
    "    a = a + b #Overlap both faded signals\n",
    "    c = np.concatenate((s1[:len(s1)-f],a,s2[f:]), axis=0)\n",
    "   \n",
    "    #For testing\n",
    "    #ipd.display(ipd.Audio(c,rate=srate))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_fade = mean_tempo/60\n",
    "delay = 0.1 # should be less than cross_fade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crsf = int(sec_convert(cross_fade))\n",
    "d = sec_convert(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_file = tracks_tstretched[0]\n",
    "playlist_file = crossfade(playlist_file,mashups[0],crsf,d)\n",
    "\n",
    "for i in range(num_files-2):\n",
    "    \n",
    "    playlist_file = crossfade(playlist_file, tracks_tstretched[i+1],crsf, d)\n",
    "    playlist_file = crossfade(playlist_file, mashups[i+1], crsf, d)\n",
    "\n",
    "playlist_file = crossfade(playlist_file, tracks_tstretched[num_files-1], crsf, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "playlist_array = np.asarray(playlist_file)\n",
    "wav.write('test_playlist_lib.wav', srate, playlist_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END ------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
